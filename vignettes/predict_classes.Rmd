---
title: "Predicting binomial and multinomial classes using semantic fingerprints, FactoMineR and glmnet"
author: "Jasper Ginn"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

In this short vignette, I'll demonstrate a simple usecase for Cortical's semantic folding API on textual data. We will use a subset of the Reuters21578 dataset, which is available through the `tm.corpus.Reuters21578` library, to predict whether an article belongs to one of several classes.

```{r, eval=TRUE, echo=FALSE}
suppressPackageStartupMessages(library(purrr))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(FinTxtUtils))
suppressPackageStartupMessages(library(ggplot2))
```

```{r, message=FALSE, eval=TRUE}
# Clean wd
rm(list=ls())
# Load data
data("fps_train")
data("fps_test")

# Unpack
train_lbls_b <- fps_train$label_binomial
train_lbls_m <- fps_train$label_multinomial
fps_train <- fps_train$fingerprints

test_lbls_b <- fps_test$label_binomial
test_lbls_m <- fps_test$label_multinomial
fps_test <- fps_test$fingerprints
```

We can plot one of the articles and get its keywords to get a better understanding of the context:

```{r}
# Select one fingerprinted document
one <- fps_train[[1]]
print(one)
```

```{r fig.height=6, fig.width=6}
# Plot
plot(one)
```

Documents can be fingerprinted in batch using `do_fingerprint_document()`. `FinTxtUtils` contains a `Collection` class that is useful for inspecting multiple documents.

```{r}
# Turn fingerprints into Collection
col_train <- Collection(fps_train)
col_test <- Collection(fps_test)
```

We can look at the intensity of each position as a way to see whether there is much overlap in the fingerprint positions. The intensity of a group of fingerprinted positions is simply the times a position occurs in all articles divided by the total number of fingerprint positions that are not 0.

```{r fig.height=6, fig.width=6}
# Plot intensity
plot(col_train) # Article fingerprints are very similar. This is not surprising given that we picked out articles about natural resources
```

There seems to be a lot of overlap in several areas. 

We can plot separate fingerprints for each category:

```{r, fig.show="hold", fig.width=3, fig.height=3}
cats <- levels(train_lbls_m)
# For each, make a collection and plot
catplots <- purrr::map(cats, function(x) {
  index <- which(train_lbls_m == x)
  col <- Collection(fps_train[index])
  plot(col) + 
    ggtitle(x)
})

for(catplot in catplots) {
  plot(catplot)
}

```

We can train a regularized logistic regression method (a 'lasso') on the data. Since the data is high-dimensional (we have many more columns than observations), this is computationally expensive. However, we can first reduce the number of dimensions using Multiple Correspondence Analysis (MCA) and then use a much smaller number of variables to train the model. We first convert the collection to a sparse binary matrix:

```{r}
# Turn test and train into a sparse binary matrix
mtrain <- as.matrix(col_train)
mtest <- as.matrix(col_test)

mtrain[1:5, 1:8]
```

To do this, we have to remove all fingerprint positions that do not occur (have a value of 0) in the training data. We do this because, should these positions exist in unseen data, we cannot predict the rotation properly.

```{r}
# Remove columns whose sums are 0
all_zero <- which(colSums(mtrain) == 0)

# Replace
mtrain_p <- mtrain[, -all_zero]
mtest_p <- mtest[, -all_zero]

# Perform multiple correspondence analysis on sparse binary train data
library(FactoMineR)

# Multiple Correspondence Analysis on train
# We have to coerce the sparse binary matrix to a
# regular matrix
train_pca <- PCA(as.matrix(mtrain_p), ncp = ncol(mtrain_p), graph = FALSE)

# Predict on test 
test_pca <- predict(train_pca, as.matrix(mtest_p))$coord

# Plot PCA (variance explained)
plotPCA <- function(eigenValue.df) {
  ggplot(data.frame(x=1:nrow(eigenValue.df),
                    y=eigenValue.df[,3]), aes(x=x, y=y)) +
    geom_line(size = 1.2) +
    theme_bw() +
    scale_x_continuous(name = "Number of principal components") +
    scale_y_continuous(name = "Cumulative percentage of variance")
}
# Plot
plotPCA(train_pca$eig)

```

For the binomial case (two test labels), we only need k=10 principal components to train a good model. In the multinomial case, we need about k=100. (you probably want to test this using cross-validation on your own data).

```{r}
# Number of princomp
k = 10

# Train glmnet model (binomial)
suppressPackageStartupMessages(library(glmnet))
# Train model
mod_train_binom <- cv.glmnet(x=data.matrix(train_pca$ind$coord[,1:k]),
                              y=train_lbls_b, family = "binomial")

# Predict
pred_class_b <- factor(
  predict(mod_train_binom, train_pca$ind$coord[,1:k], type="class")
, levels = c("other", "crude")
)

# Confusion matrix
caret::confusionMatrix(pred_class_b, train_lbls_b, positive = "crude")
```

```{r}
k = 10
library(ggplot2)
plot_df <- data_frame(
  "prob" = predict(mod_train_binom, train_pca$ind$coord[,1:k], type="response")[,1],
  "class" = train_lbls_b,
  "class_pred" = pred_class_b
)

# Plot double-density plot for binomial case
ggplot(plot_df, aes(x= prob, group = class, linetype=class)) +
  geom_density() +
  theme_bw()
```

Using multinomial labels:

```{r}
# Number of components
k <- 100

# Model
mod_train_multi <- cv.glmnet(x=data.matrix(train_pca$ind$coord[,1:k]),
                              y=train_lbls_m, family = "multinomial")

# Predict
pred_class_m <- factor(
  predict(mod_train_multi, train_pca$ind$coord[,1:k], type="class"),
  levels = levels(train_lbls_m)
)

# Confusion matrix
caret::confusionMatrix(pred_class_m, train_lbls_m)
```

In both cases, we obtain a good result.

On test set:

```{r}
# Number of components
k <- 100

# Predict
pred_class_m_t <- factor(
  predict(mod_train_multi, test_pca[,1:k], type="class"),
  levels = levels(test_lbls_m)
)

# Confusion matrix
caret::confusionMatrix(pred_class_m_t, test_lbls_m)
```
